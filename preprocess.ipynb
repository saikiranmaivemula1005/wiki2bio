{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess.ipynb","provenance":[],"authorship_tag":"ABX9TyMafT4c0Jk3omMFnnL12Jpz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"B_vkHMprnucw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"error","timestamp":1592388753664,"user_tz":-330,"elapsed":3176,"user":{"displayName":"Sai Kiranmai Vemula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYzW8_0OHzEcN1utqXxk21oTbCtmfOp4zz4HpDFw=s64","userId":"15355417407830951385"}},"outputId":"0500980b-a625-4f7d-c2ac-c8fad0d832a1"},"source":["import re, time, os\n","\n","def split_infobox():\n","    \"\"\"\n","    extract box content, field type and position information from infoboxes from original_data\n","    *.box.val is the box content (token)\n","    *.box.lab is the field type for each token\n","    *.box.pos is the position counted from the begining of a field\n","    \"\"\"\n","    bwfile = [\"processed_data/train/train.box.val\", \n","              \"processed_data/valid/valid.box.val\", \n","              \"processed_data/test/test.box.val\"]\n","    bffile = [\"processed_data/train/train.box.lab\", \n","              \"processed_data/valid/valid.box.lab\", \n","              \"processed_data/test/test.box.lab\"]\n","    bpfile = [\"processed_data/train/train.box.pos\", \n","              \"processed_data/valid/valid.box.pos\", \n","              \"processed_data/test/test.box.pos\"]\n","    boxes = [\"original_data/train.box\", \"original_data/valid.box\", \"original_data/test.box\"]\n","    \n","    mixb_word, mixb_label, mixb_pos = [], [], []\n","    for fboxes in boxes:\n","        box = open(fboxes, \"r\", encoding='utf-8').read().strip().split('\\n')\n","        box_word, box_label, box_pos = [], [], []\n","        for ib in box:\n","            item = ib.split('\\t')\n","            box_single_word, box_single_label, box_single_pos = [], [], []\n","            for it in item:\n","                if len(it.split(':')) > 2:\n","                    continue\n","                # print it\n","                prefix, word = it.split(':')\n","                if '<none>' in word or word.strip()=='' or prefix.strip()=='':\n","                    continue\n","                new_label = re.sub(\"_[1-9]\\d*$\", \"\", prefix)\n","                if new_label.strip() == \"\":\n","                    continue\n","                box_single_word.append(word)\n","                box_single_label.append(new_label)\n","                if re.search(\"_[1-9]\\d*$\", prefix):\n","                    field_id = int(prefix.split('_')[-1])\n","                    box_single_pos.append(field_id if field_id<=30 else 30)\n","                else:\n","                    box_single_pos.append(1)\n","            box_word.append(box_single_word)\n","            box_label.append(box_single_label)\n","            box_pos.append(box_single_pos)\n","        mixb_word.append(box_word)\n","        mixb_label.append(box_label)\n","        mixb_pos.append(box_pos)\n","    for k, m in enumerate(mixb_word):\n","        with open(bwfile[k], \"w+\") as h:\n","            for items in m:\n","                for sens in items:\n","                    h.write(str(sens).encode(\"utf-8\") + \" \")\n","                h.write('\\n')\n","    for k, m in enumerate(mixb_label):\n","        with open(bffile[k], \"w+\") as h:\n","            for items in m:\n","                for sens in items:\n","                    h.write(str(sens) + \" \")\n","                h.write('\\n')\n","    for k, m in enumerate(mixb_pos):\n","        with open(bpfile[k], \"w+\") as h:\n","            for items in m:\n","                for sens in items:\n","                    h.write(str(sens) + \" \")\n","                h.write('\\n')\n","\n","def reverse_pos():\n","    # get the position counted from the end of a field\n","    bpfile = [\"processed_data/train/train.box.pos\", \"processed_data/valid/valid.box.pos\", \"processed_data/test/test.box.pos\"]\n","    bwfile = [\"processed_data/train/train.box.rpos\", \"processed_data/valid/valid.box.rpos\", \"processed_data/test/test.box.rpos\"]\n","    for k, pos in enumerate(bpfile):\n","        box = open(pos, \"r\").read().strip().split('\\n')\n","        reverse_pos = []\n","        for bb in box:\n","            pos = bb.split()\n","            tmp_pos = []\n","            single_pos = []\n","            for p in pos:\n","                if int(p) == 1 and len(tmp_pos) != 0:\n","                    single_pos.extend(tmp_pos[::-1])\n","                    tmp_pos = []\n","                tmp_pos.append(p)\n","            single_pos.extend(tmp_pos[::-1])\n","            reverse_pos.append(single_pos)\n","        with open(bwfile[k], 'w+') as bw:\n","            for item in reverse_pos:\n","                bw.write(\" \".join(item) + '\\n')\n","\n","def check_generated_box():\n","    ftrain = [\"processed_data/train/train.box.val\",\n","              \"processed_data/train/train.box.lab\",\n","              \"processed_data/train/train.box.pos\",\n","              \"processed_data/train/train.box.rpos\"]\n","    ftest  = [\"processed_data/test/test.box.val\", \n","              \"processed_data/test/test.box.lab\",\n","              \"processed_data/test/test.box.pos\",\n","              \"processed_data/test/test.box.rpos\"]\n","    fvalid = [\"processed_data/valid/valid.box.val\", \n","              \"processed_data/valid/valid.box.lab\", \n","              \"processed_data/valid/valid.box.pos\",\n","              \"processed_data/valid/valid.box.rpos\"]\n","    for case in [ftrain, ftest, fvalid]:\n","        vals = open(case[0], 'r').read().strip().split('\\n')\n","        labs = open(case[1], 'r').read().strip().split('\\n')\n","        poses = open(case[2], 'r').read().strip().split('\\n')\n","        rposes = open(case[3], 'r').read().strip().split('\\n')\n","        assert len(vals) == len(labs)\n","        assert len(poses) == len(labs)\n","        assert len(rposes) == len(poses)\n","        for val, lab, pos, rpos in zip(vals, labs, poses, rposes):\n","            vval = val.strip().split(' ')\n","            llab = lab.strip().split(' ')\n","            ppos = pos.strip().split(' ')\n","            rrpos = rpos.strip().split(' ')\n","            if len(vval) != len(llab) or len(llab) != len(ppos) or len(ppos) != len(rrpos):\n","                print (case)\n","                print (val)\n","                print (len(vval))\n","                print (len(llab))\n","                print (len(ppos))\n","                print (len(rrpos))\n","            assert len(vval) == len(llab)\n","            assert len(llab) == len(ppos)\n","            assert len(ppos) == len(rrpos)\n","\n","\n","def split_summary_for_rouge():\n","    bpfile = [\"original_data/test.summary\", \"original_data/valid.summary\"]\n","    bwfile = [\"processed_data/test/test_split_for_rouge/\", \"processed_data/valid/valid_split_for_rouge/\"]\n","    for i, fi in enumerate(bpfile):\n","        fread = open(fi, 'r')\n","        k = 0\n","        for line in fread:\n","            with open(bwfile[i] + 'gold_summary_' + str(k), 'w') as sw:\n","                sw.write(line.strip() + '\\n')\n","            k += 1\n","        fread.close()\n","\n","\n","\n","class Vocab(object):\n","    \"\"\"vocabulary for words and field types\"\"\"\n","    def __init__(self):\n","        vocab = dict()\n","        vocab['PAD'] = 0\n","        vocab['START_TOKEN'] = 1\n","        vocab['END_TOKEN'] = 2\n","        vocab['UNK_TOKEN'] = 3\n","        cnt = 4\n","        with open(\"original_data/word_vocab.txt\", \"r\") as v:\n","            for line in v:\n","                word = line.strip().split()[0]\n","                vocab[word] = cnt\n","                cnt += 1\n","        self._word2id = vocab\n","        self._id2word = {value: key for key, value in vocab.items()}\n","\n","        key_map = dict()\n","        key_map['PAD'] = 0\n","        key_map['START_TOKEN'] = 1\n","        key_map['END_TOKEN'] = 2\n","        key_map['UNK_TOKEN'] = 3\n","        cnt = 4\n","        with open(\"original_data/field_vocab.txt\", \"r\") as v:\n","            for line in v:\n","                key = line.strip().split()[0]\n","                key_map[key] = cnt\n","                cnt += 1\n","        self._key2id = key_map\n","        self._id2key = {value: key for key, value in key_map.items()}\n","\n","    def word2id(self, word):\n","        ans = self._word2id[word] if word in self._word2id else 3\n","        return ans\n","\n","    def id2word(self, id):\n","        ans = self._id2word[int(id)]\n","        return ans\n","\n","    def key2id(self, key):\n","        ans = self._key2id[key] if key in self._key2id else 3\n","        return ans\n","\n","    def id2key(self, id):\n","        ans = self._id2key[int(id)]\n","        return ans\n","\n","def table2id():\n","    fvals = ['processed_data/train/train.box.val',\n","             'processed_data/test/test.box.val',\n","             'processed_data/valid/valid.box.val']\n","    flabs = ['processed_data/train/train.box.lab',\n","             'processed_data/test/test.box.lab',\n","             'processed_data/valid/valid.box.lab']\n","    fsums = ['original_data/train.summary',\n","             'original_data/test.summary',\n","             'original_data/valid.summary']\n","    fvals2id = ['processed_data/train/train.box.val.id',\n","                'processed_data/test/test.box.val.id',\n","                'processed_data/valid/valid.box.val.id']\n","    flabs2id = ['processed_data/train/train.box.lab.id',\n","                'processed_data/test/test.box.lab.id',\n","                'processed_data/valid/valid.box.lab.id']\n","    fsums2id = ['processed_data/train/train.summary.id',\n","                'processed_data/test/test.summary.id',\n","                'processed_data/valid/valid.summary.id']\n","    vocab = Vocab()\n","    for k, ff in enumerate(fvals):\n","        fi = open(ff, 'r')\n","        fo = open(fvals2id[k], 'w')\n","        for line in fi:\n","            items = line.strip().split()\n","            fo.write(\" \".join([str(vocab.word2id(word)) for word in items]) + '\\n')\n","        fi.close()\n","        fo.close()\n","    for k, ff in enumerate(flabs):\n","        fi = open(ff, 'r')\n","        fo = open(flabs2id[k], 'w')\n","        for line in fi:\n","            items = line.strip().split()\n","            fo.write(\" \".join([str(vocab.key2id(key)) for key in items]) + '\\n')\n","        fi.close()\n","        fo.close()\n","    for k, ff in enumerate(fsums):\n","        fi = open(ff, 'r')\n","        fo = open(fsums2id[k], 'w')\n","        for line in fi:\n","            items = line.strip().split()\n","            fo.write(\" \".join([str(vocab.word2id(word)) for word in items]) + '\\n')\n","        fi.close()\n","        fo.close()\n","\n","\n","def preprocess():\n","    \"\"\"\n","    We use a triple <f, p+, p-> to represent the field information of a token in the specific field. \n","    p+&p- are the position of the token in that field counted from the begining and the end of the field.\n","    For example, for a field (birthname, Jurgis Mikelatitis) in an infoboxes, we represent the field as\n","    (Jurgis, <birthname, 1, 2>) & (Mikelatitis, <birthname, 2, 1>)\n","    \"\"\"\n","    print(\"extracting token, field type and position info from original data ...\")\n","    time_start = time.time()\n","    split_infobox()\n","    reverse_pos()\n","    duration = time.time() - time_start\n","    print(\"extract finished in %.3f seconds\" % float(duration))\n","\n","    print(\"spliting test and valid summaries for ROUGE evaluation ...\")\n","    time_start = time.time()\n","    split_summary_for_rouge()\n","    duration = time.time() - time_start\n","    print(\"split finished in %.3f seconds\" % float(duration))\n","\n","    print(\"turning words and field types to ids ...\")\n","    time_start = time.time()\n","    table2id()\n","    duration = time.time() - time_start\n","    print(\"idlization finished in %.3f seconds\" % float(duration))\n","\n","\n","def make_dirs():\n","    os.mkdir(\"results/\")\n","    os.mkdir(\"results/res/\")\n","    os.mkdir(\"results/evaluation/\")\n","    os.mkdir(\"processed_data/\")\n","    os.mkdir(\"processed_data/train/\")\n","    os.mkdir(\"processed_data/test/\")\n","    os.mkdir(\"processed_data/valid/\")\n","    os.mkdir(\"processed_data/test/test_split_for_rouge/\")\n","    os.mkdir(\"processed_data/valid/valid_split_for_rouge/\")\n","\n","if __name__ == '__main__':\n","    make_dirs()\n","    preprocess()\n","    check_generated_box()\n","    print(\"check done\")\n"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileExistsError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8c23270242ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mmake_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mcheck_generated_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-8c23270242ad>\u001b[0m in \u001b[0;36mmake_dirs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/res/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/evaluation/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'results/'"]}]}]}