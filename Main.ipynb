{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","provenance":[],"authorship_tag":"ABX9TyNZwvcywYJwiLGpadxblGqS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3duk2-WlsA7E","colab_type":"code","outputId":"9b3dae7c-142c-4c91-da2e-f16a93de5b7d","executionInfo":{"status":"error","timestamp":1592286915110,"user_tz":-330,"elapsed":1732,"user":{"displayName":"Sai Kiranmai Vemula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYzW8_0OHzEcN1utqXxk21oTbCtmfOp4zz4HpDFw=s64","userId":"15355417407830951385"}},"colab":{"base_uri":"https://localhost:8080/","height":362}},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","# @Time    : 17-4-27 下午8:44\n","# @Author  : Tianyu Liu\n","\n","import sys\n","import os\n","import tensorflow as tf\n","import time\n","from SeqUnit import *\n","from DataLoader import DataLoader\n","import numpy as np\n","from PythonROUGE import PythonROUGE\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from preprocess import *\n","from util import * \n","\n","\n","tf.app.flags.DEFINE_integer(\"hidden_size\", 500, \"Size of each layer.\")\n","tf.app.flags.DEFINE_integer(\"emb_size\", 400, \"Size of embedding.\")\n","tf.app.flags.DEFINE_integer(\"field_size\", 50, \"Size of embedding.\")\n","tf.app.flags.DEFINE_integer(\"pos_size\", 5, \"Size of embedding.\")\n","tf.app.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size of train set.\")\n","tf.app.flags.DEFINE_integer(\"epoch\", 50, \"Number of training epoch.\")\n","tf.app.flags.DEFINE_integer(\"source_vocab\", 20003,'vocabulary size')\n","tf.app.flags.DEFINE_integer(\"field_vocab\", 1480,'vocabulary size')\n","tf.app.flags.DEFINE_integer(\"position_vocab\", 31,'vocabulary size')\n","tf.app.flags.DEFINE_integer(\"target_vocab\", 20003,'vocabulary size')\n","tf.app.flags.DEFINE_integer(\"report\", 5000,'report valid results after some steps')\n","tf.app.flags.DEFINE_float(\"learning_rate\", 0.0003,'learning rate')\n","\n","tf.app.flags.DEFINE_string(\"mode\",'train','train or test')\n","tf.app.flags.DEFINE_string(\"load\",'0','load directory') # BBBBBESTOFAll\n","tf.app.flags.DEFINE_string(\"dir\",'processed_data','data set directory')\n","tf.app.flags.DEFINE_integer(\"limits\", 0,'max data set size')\n","\n","\n","tf.app.flags.DEFINE_boolean(\"dual_attention\", True,'dual attention layer or normal attention')\n","tf.app.flags.DEFINE_boolean(\"fgate_encoder\", True,'add field gate in encoder lstm')\n","\n","tf.app.flags.DEFINE_boolean(\"field\", False,'concat field information to word embedding')\n","tf.app.flags.DEFINE_boolean(\"position\", False,'concat position information to word embedding')\n","tf.app.flags.DEFINE_boolean(\"encoder_pos\", True,'position information in field-gated encoder')\n","tf.app.flags.DEFINE_boolean(\"decoder_pos\", True,'position information in dual attention decoder')\n","\n","\n","FLAGS = tf.app.flags.FLAGS\n","last_best = 0.0\n","\n","gold_path_test = 'processed_data/test/test_split_for_rouge/gold_summary_'\n","gold_path_valid = 'processed_data/valid/valid_split_for_rouge/gold_summary_'\n","\n","# test phase\n","if FLAGS.load != \"0\":\n","    save_dir = 'results/res/' + FLAGS.load + '/'\n","    save_file_dir = save_dir + 'files/'\n","    pred_dir = 'results/evaluation/' + FLAGS.load + '/'\n","    if not os.path.exists(pred_dir):\n","        os.mkdir(pred_dir)\n","    if not os.path.exists(save_file_dir):\n","        os.mkdir(save_file_dir)\n","    pred_path = pred_dir + 'pred_summary_'\n","    pred_beam_path = pred_dir + 'beam_summary_'\n","# train phase\n","else:\n","    prefix = str(int(time.time() * 1000))\n","    save_dir = 'results/res/' + prefix + '/'\n","    save_file_dir = save_dir + 'files/'\n","    pred_dir = 'results/evaluation/' + prefix + '/'\n","    os.mkdir(save_dir)\n","    if not os.path.exists(pred_dir):\n","        os.mkdir(pred_dir)\n","    if not os.path.exists(save_file_dir):\n","        os.mkdir(save_file_dir)\n","    pred_path = pred_dir + 'pred_summary_'\n","    pred_beam_path = pred_dir + 'beam_summary_'\n","\n","log_file = save_dir + 'log.txt'\n","\n","\n","def train(sess, dataloader, model):\n","    write_log(\"#######################################################\")\n","    for flag in FLAGS.__flags:\n","        write_log(flag + \" = \" + str(FLAGS.__flags[flag]))\n","    write_log(\"#######################################################\")\n","    trainset = dataloader.train_set\n","    k = 0\n","    loss, start_time = 0.0, time.time()\n","    for _ in range(FLAGS.epoch):\n","        for x in dataloader.batch_iter(trainset, FLAGS.batch_size, True):\n","            loss += model(x, sess)\n","            k += 1\n","            progress_bar(k%FLAGS.report, FLAGS.report)\n","            if (k % FLAGS.report == 0):\n","                cost_time = time.time() - start_time\n","                write_log(\"%d : loss = %.3f, time = %.3f \" % (k // FLAGS.report, loss, cost_time))\n","                loss, start_time = 0.0, time.time()\n","                if k // FLAGS.report >= 1: \n","                    ksave_dir = save_model(model, save_dir, k // FLAGS.report)\n","                    write_log(evaluate(sess, dataloader, model, ksave_dir, 'valid'))\n","                    \n","\n","\n","def test(sess, dataloader, model):\n","    evaluate(sess, dataloader, model, save_dir, 'test')\n","\n","def save_model(model, save_dir, cnt):\n","    new_dir = save_dir + 'loads' + '/' \n","    if not os.path.exists(new_dir):\n","        os.mkdir(new_dir)\n","    nnew_dir = new_dir + str(cnt) + '/'\n","    if not os.path.exists(nnew_dir):\n","        os.mkdir(nnew_dir)\n","    model.save(nnew_dir)\n","    return nnew_dir\n","\n","def evaluate(sess, dataloader, model, ksave_dir, mode='valid'):\n","    if mode == 'valid':\n","        # texts_path = \"original_data/valid.summary\"\n","        texts_path = \"processed_data/valid/valid.box.val\"\n","        gold_path = gold_path_valid\n","        evalset = dataloader.dev_set\n","    else:\n","        # texts_path = \"original_data/test.summary\"\n","        texts_path = \"processed_data/test/test.box.val\"\n","        gold_path = gold_path_test\n","        evalset = dataloader.test_set\n","    \n","    # for copy words from the infoboxes\n","    texts = open(texts_path, 'r').read().strip().split('\\n')\n","    texts = [list(t.strip().split()) for t in texts]\n","    v = Vocab()\n","\n","    # with copy\n","    pred_list, pred_list_copy, gold_list = [], [], []\n","    pred_unk, pred_mask = [], []\n","    \n","    k = 0\n","    for x in dataloader.batch_iter(evalset, FLAGS.batch_size, False):\n","        predictions, atts = model.generate(x, sess)\n","        atts = np.squeeze(atts)\n","        idx = 0\n","        for summary in np.array(predictions):\n","            with open(pred_path + str(k), 'w') as sw:\n","                summary = list(summary)\n","                if 2 in summary:\n","                    summary = summary[:summary.index(2)] if summary[0] != 2 else [2]\n","                real_sum, unk_sum, mask_sum = [], [], []\n","                for tk, tid in enumerate(summary):\n","                    if tid == 3:\n","                        sub = texts[k][np.argmax(atts[tk,: len(texts[k]),idx])]\n","                        real_sum.append(sub)\n","                        mask_sum.append(\"**\" + str(sub) + \"**\")\n","                    else:\n","                        real_sum.append(v.id2word(tid))\n","                        mask_sum.append(v.id2word(tid))\n","                    unk_sum.append(v.id2word(tid))\n","                sw.write(\" \".join([str(x) for x in real_sum]) + '\\n')\n","                pred_list.append([str(x) for x in real_sum])\n","                pred_unk.append([str(x) for x in unk_sum])\n","                pred_mask.append([str(x) for x in mask_sum])\n","                k += 1\n","                idx += 1\n","    write_word(pred_mask, ksave_dir, mode + \"_summary_copy.txt\")\n","    write_word(pred_unk, ksave_dir, mode + \"_summary_unk.txt\")\n","\n","\n","    for tk in range(k):\n","        with open(gold_path + str(tk), 'r') as g:\n","            gold_list.append([g.read().strip().split()])\n","\n","    gold_set = [[gold_path + str(i)] for i in range(k)]\n","    pred_set = [pred_path + str(i) for i in range(k)]\n","\n","    recall, precision, F_measure = PythonROUGE(pred_set, gold_set, ngram_order=4)\n","    bleu = corpus_bleu(gold_list, pred_list)\n","    copy_result = \"with copy F_measure: %s Recall: %s Precision: %s BLEU: %s\\n\" % \\\n","    (str(F_measure), str(recall), str(precision), str(bleu))\n","    # print copy_result\n","\n","    for tk in range(k):\n","        with open(pred_path + str(tk), 'w') as sw:\n","            sw.write(\" \".join(pred_unk[tk]) + '\\n')\n","\n","    recall, precision, F_measure = PythonROUGE(pred_set, gold_set, ngram_order=4)\n","    bleu = corpus_bleu(gold_list, pred_unk)\n","    nocopy_result = \"without copy F_measure: %s Recall: %s Precision: %s BLEU: %s\\n\" % \\\n","    (str(F_measure), str(recall), str(precision), str(bleu))\n","    # print nocopy_result\n","    result = copy_result + nocopy_result \n","    # print result\n","    if mode == 'valid':\n","        print (result)\n","\n","    return result\n","\n","\n","\n","def write_log(s):\n","    print (s)\n","    with open(log_file, 'a') as f:\n","        f.write(s+'\\n')\n","\n","\n","def main():\n","    config = tf.ConfigProto(allow_soft_placement=True)\n","    config.gpu_options.allow_growth = True\n","    with tf.Session(config=config) as sess:\n","        copy_file(save_file_dir)\n","        dataloader = DataLoader(FLAGS.dir, FLAGS.limits)\n","        model = SeqUnit(batch_size=FLAGS.batch_size, hidden_size=FLAGS.hidden_size, emb_size=FLAGS.emb_size,\n","                        field_size=FLAGS.field_size, pos_size=FLAGS.pos_size, field_vocab=FLAGS.field_vocab,\n","                        source_vocab=FLAGS.source_vocab, position_vocab=FLAGS.position_vocab,\n","                        target_vocab=FLAGS.target_vocab, scope_name=\"seq2seq\", name=\"seq2seq\",\n","                        field_concat=FLAGS.field, position_concat=FLAGS.position,\n","                        fgate_enc=FLAGS.fgate_encoder, dual_att=FLAGS.dual_attention, decoder_add_pos=FLAGS.decoder_pos,\n","                        encoder_add_pos=FLAGS.encoder_pos, learning_rate=FLAGS.learning_rate)\n","        sess.run(tf.global_variables_initializer())\n","        # copy_file(save_file_dir)\n","        if FLAGS.load != '0':\n","            model.load(save_dir)\n","        if FLAGS.mode == 'train':\n","            train(sess, dataloader, model)\n","        else:\n","            test(sess, dataloader, model)\n","\n","\n","if __name__=='__main__':\n","    # with tf.device('/gpu:' + FLAGS.gpu):\n","    main()"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e34ec45ed6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSeqUnit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SeqUnit'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}